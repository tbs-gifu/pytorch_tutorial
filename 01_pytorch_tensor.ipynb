{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Pytorch tensor\n",
    "#### ＝＝＝ 目次 ＝＝＝\n",
    "0. Pytorchの呼び出し\n",
    "1. Tensorの生成\n",
    "2. 基本演算\n",
    "3. 誤差逆伝播&勾配降下法\n",
    "4. GPUの使用\n",
    "\n",
    "## Pytorchの特徴\n",
    "- Numpyに代わってGPU上で動くパッケージ\n",
    "- 柔軟性があり高速な深層学習のプラットフォーム\n",
    "- define by run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchはCUDAなどのバージョンを見ながら,インストールする必要がある\n",
    "\n",
    "公式HP：https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Pytorchの呼び出し\n",
    "`torch`という名前のモジュールをインポートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Tensor(テンソル)の生成\n",
    "tensor：多次元配列のようなもの(ex. ベクトル，行列)\n",
    "\n",
    "Numpyのndarrayのようなもの(ndarrayと比べて，GPUを使うことで計算を高速化できる．また，勾配情報を保持できる．)\n",
    "\n",
    "|<div align='center'>関数</div>|<div align='center'>意味</div>|<div align='center'>例</div>|\n",
    "|---|---|---|\n",
    "|<div align='left'>torch.tensor(array)</div>|<div align='left'>配列をtensorに変換</div>|<div align='left'>torch.tensor([2.5, 5.0, 3.6])</div>|\n",
    "|<div align='left'>torch.empty(shape)</div>|<div align='left'>空のテンソルを作成 (何かしらの値が入っている)</div>|<div align='left'>torch.empty(2, 5)</div>|\n",
    "|<div align='left'>torch.zeros(shape)</div>|<div align='left'>0のテンソルを作成</div>|<div align='left'>torch.zeros(2, 5)</div>|\n",
    "|<div align='left'>torch.ones(shape)</div>|<div align='left'>1のテンソルを作成</div>|<div align='left'>torch.ones(2, 5)</div>|\n",
    "|<div align='left'>torch.full(shape,fill_value)</div>|<div align='left'>任意の値のテンソルを作成</div>|<div align='left'>torch.full((2, 5),fill_value=4)</div>|\n",
    "|<div align='left'>torch.zeros_like(tensor)</div>|<div align='left'>引数のテンソルと同じサイズの0のテンソルを作成</div>|<div align='left'>torch.zeros_like(a)</div>|\n",
    "|<div align='left'>torch.eye(shape)</div>|<div align='left'>単位行列を作成</div>|<div align='left'>torch.eye(3, 3)</div>|\n",
    "|<div align='left'>torch.rand(shape)</div>|<div align='left'>[0, 1]の一様分布による乱数</div>|<div align='left'>torch.rand(2, 5)</div>|\n",
    "|<div align='left'>torch.randn(shape)</div>|<div align='left'>標準正規分布による乱数</div>|<div align='left'>torch.randn(2, 5)</div>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor()`：配列をtensorに変換\n",
    "\n",
    "`dtype`で値のデータ型を指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次元配列\n",
    "x = torch.tensor([5.5, 3, 2.4])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二次元配列\n",
    "x = torch.tensor([[3, 5, 2],\n",
    "                  [8, 8, 1],\n",
    "                  [4, 1, 5],\n",
    "                  [5, 8, 8],\n",
    "                  [2, 5, 2]], dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.rand()`：[0, 1]の一様分布による乱数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.randn()`：標準正規分布による乱数 (平均0, 分散1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## tensorのshape\n",
    "`変数.shape` or `変数.size()`：tensorのshapeを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 6)\n",
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`変数.view()`：tensorのshapeを変更したものを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # -1を使うと自動で調整してくれる\n",
    "print(\"x :\", x.shape)\n",
    "print(\"y :\", y.shape)\n",
    "print(\"z :\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`変数.squeeze(dim)`：指定した次元のサイズが1の場合削除、dimの指定がなければサイズ1をすべて削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 28, 28, 1)\n",
    "y = x.squeeze(dim=0)\n",
    "z = x.squeeze()\n",
    "print(\"x :\", x.shape)\n",
    "print(\"y :\", y.shape)\n",
    "print(\"z :\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`変数.unsqueeze(dim)`：指定した位置にサイズ1の次元を挿入したtensorを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(28, 28)\n",
    "y = x.unsqueeze(dim=0)\n",
    "z = x.unsqueeze(dim=1)\n",
    "print(\"x :\", x.shape)\n",
    "print(\"y :\", y.shape)\n",
    "print(\"z :\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## スライス\n",
    "リストやndarrayのように，スライスを用いることで一部を抽出できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 6)\n",
    "print(x)\n",
    "print(x[1:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Numpyとの変換\n",
    "`変数.numpy()`：tensor → ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 3)\n",
    "print(type(a))\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(type(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.from_numpy(ndarray)`：ndarray → tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "print(type(a))\n",
    "print(a)\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "print(type(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 基本演算\n",
    "\n",
    "|<div align='center'>演算</div>|<div align='center'>演算子</div>|\n",
    "|---|---|\n",
    "|<div align='center'>足し算</div>|<div align='center'>+</div>|\n",
    "|<div align='center'>引き算</div>|<div align='center'>-</div>|\n",
    "|<div align='center'>アダマール積</div>|<div align='center'>*</div>|\n",
    "|<div align='center'>行列積$^{*1}$</div>|<div align='center'>torch.matmul()</div>|\n",
    "\n",
    "$^{*1}$ Pytorchにはそれぞれのshapeのtensorに合わせた積の関数(`dot`や`mm`など)があるが，`matmul`は任意のshapeのtensorに対する汎用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルの作成\n",
    "x = torch.tensor([[4., 3.], \n",
    "                  [2., 1.]])\n",
    "y = torch.tensor([[2., 2.], \n",
    "                  [1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 足し算\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アダマール積\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行列積\n",
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1次元 × 1次元 -> 0次元(スカラー)\n",
    "x = torch.randn(3)\n",
    "y = torch.randn(3)\n",
    "z = torch.matmul(x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2次元 × 1次元 -> 1次元(ベクトル)\n",
    "x = torch.randn(4, 3)\n",
    "y = torch.randn(3)\n",
    "z = torch.matmul(x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2次元 × 2次元 -> 2次元(行列)\n",
    "x = torch.randn(4, 3)\n",
    "y = torch.randn(3, 5)\n",
    "z = torch.matmul(x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3次元 × 2次元 -> 3次元(テンソル)\n",
    "x = torch.randn(100, 4, 3)\n",
    "y = torch.randn(3, 5)\n",
    "z = torch.matmul(x, y)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他の演算\n",
    "\n",
    "|<div align='center'>演算</div>|<div align='center'>関数</div>|\n",
    "|---|---|\n",
    "|<div align='center'>要素の和</div>|<div align='center'>torch.sum(tensor, dim)</div>|\n",
    "|<div align='center'>要素の平均</div>|<div align='center'>torch.mean(tensor, dim)</div>|\n",
    "|<div align='center'>要素の標準偏差</div>|<div align='center'>torch.std(tensor, dim)</div>|\n",
    "|<div align='center'>要素の最大値</div>|<div align='center'>torch.max(tensor, dim)</div>|\n",
    "|<div align='center'>要素の最小値</div>|<div align='center'>torch.min(tensor, dim)</div>|\n",
    "|<div align='center'>tensorの結合</div>|<div align='center'>torch.cat(tensors, dim)</div>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要素の和\n",
    "x = torch.ones(4, 3)\n",
    "print(torch.sum(x))\n",
    "print(torch.sum(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要素の最大値\n",
    "x = torch.rand(2, 5)\n",
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max関数の`dim`を指定した場合，最大値を取るインデックスも返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100, 10)\n",
    "max_values, indices = torch.max(x, dim=1)\n",
    "\n",
    "print(max_values.shape)\n",
    "print(max_values)\n",
    "print(indices.shape)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cat(tensors, dim)`：指定した次元に対して，tensorを結合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(100, 3, 10, 10)\n",
    "tensor2 = torch.randn(100, 3, 10, 10)\n",
    "\n",
    "torch.cat([tensor1, tensor2], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(100, 3, 10, 10)\n",
    "tensor2 = torch.randn(100, 3, 10, 10)\n",
    "\n",
    "torch.cat([tensor1, tensor2], dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10個のtensor(3, 32, 32)を一つのtensor(10, 3, 32, 32)にまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10個のtensor(3, 32, 32)のリスト\n",
    "tensors = [torch.randn(3, 32, 32) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = torch.cat([tensor.unsqueeze(0) for tensor in tensors], dim=0)\n",
    "tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 誤差逆伝播&勾配降下法\n",
    "`変数.backward()`：backpropagation(誤差逆伝播)による微分を行う．\n",
    "\n",
    "`requires_grad=True`を指定することでtensorの勾配を保持する．\n",
    "\n",
    "例.\n",
    "$$z=x^2 + \\frac{y^2}{2}$$\n",
    "$$\\frac{\\partial z}{\\partial x} = 2x, \\frac{\\partial z}{\\partial y} = y$$\n",
    "$$\\frac{\\partial z}{\\partial x}|_{x=1.0} = 2.0, \\frac{\\partial z}{\\partial y}|_{y=1.0} = 1.0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = x * x + y * y / 2 \n",
    "print(\"x :\", x)\n",
    "print(\"y :\", y)\n",
    "print(\"z :\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "z.backward()\n",
    "print(\"xの勾配 :\", x.grad)\n",
    "print(\"yの勾配 :\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizerによる勾配降下法\n",
    "backpropagationで求めた勾配を用いて，optimizerによりtensorの値を更新する(学習)\n",
    "\n",
    "例. $z=x^2 + \\frac{y^2}{2}$が最小となるときの$(x,y)$を勾配降下法で求める．初期値 $(x,y)=(1.0,1.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期値1のパラメータ\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizerを定義\n",
    "# 更新するパラメータや学習率などを指定\n",
    "optimizer = optim.SGD([x, y], lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDによる更新を1回行う\n",
    "$$x=x-lr\\frac{\\partial z}{\\partial x}, y=y-lr\\frac{\\partial z}{\\partial y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # 勾配を初期化\n",
    "z = x*x + y*y/2       # 順伝播\n",
    "z.backward()          # backpropagation\n",
    "optimizer.step()      # 勾配を元にパラメータを更新\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続けて更新を20回行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20回パラメータを更新\n",
    "for i in range(1, 21):\n",
    "    optimizer.zero_grad() # 勾配を初期化\n",
    "    z = x*x + y*y/2       # 順伝播\n",
    "    z.backward()          # backpropagation\n",
    "    optimizer.step()      # 勾配を元にパラメータを更新\n",
    "\n",
    "    print(i, ':', x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z=x^2 + \\frac{y^2}{2}$が最小となるときの$(x,y)=(0,0)$に近づいてることが分かる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GPUの使用\n",
    "`torch.cuda.is_available()`：GPU(cuda)が使用できる場合`True`を，できない場合`False`を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用しているデバイスをdeviceに代入する\n",
    "\n",
    "cudaが使用可能であれば`\"cuda:0\"`，そうでなければ`\"cpu\"`を指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス：\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`変数.to(device)`：変数をデバイス(cuda or cpu)に渡す\n",
    "\n",
    "これによりデバイス上で計算が可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4)\n",
    "y = torch.randn(4)\n",
    "\n",
    "# tensorをGPUへ\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "z = x + y # GPU上で計算が行われる\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数をCPUに渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.to(\"cpu\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## マルチGPUの使用について\n",
    "GPUが複数使用できる場合，`torch.nn.DataParallel`をモデルに適用することで並列計算が行える．\n",
    "\n",
    "使用できるGPUの個数は`torch.cuda.device_count()`で確認できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelを定義した後に記述\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークについては`02_pytorch_network.ipynb`で説明する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
